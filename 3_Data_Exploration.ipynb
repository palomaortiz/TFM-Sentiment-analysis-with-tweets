{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "On the occasion of the Womens day on March 8th, 2019, tweets were collected from Twitter to create the dataset with which the sentiment analysis of this work will be carried out.\n",
    "\n",
    "This notebook will explore the dataset to get information about the event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries and configuring some settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "pd.set_option('max_colwidth',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the processed data from the input csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ejecuciones_codigo/3_preprocessed_tweets_8M.csv\", sep=\"|\", lineterminator='\\n', low_memory=False)\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to `Created date` fields, this is the first tweet extracted from the protest day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tweet = df.sort_values(by=[\"created_date\"],ascending=True)[[\"tweets\", \"created_date\", \"source\", \"RTs\", \"user_name\", \"user_location\"]].head(1).iloc[0]\n",
    "first_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "This is the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_tweet = df.sort_values(by=[\"created_date\"],ascending=True)[[\"tweets\", \"created_date\", \"source\", \"RTs\", \"user_name\", \"user_location\"]].tail(1).iloc[0]\n",
    "last_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the tweet with most retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"RTs\"],ascending=False)[[\"tweets\", \"created_date\", \"source\", \"RTs\", \"user_name\", \"user_location\"]].head(1).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the tweets with more likes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"likes\"],ascending=False)[[\"tweets\", \"created_date\", \"source\", \"RTs\", \"likes\", \"user_name\", \"user_location\"]].head(1).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the Top10 of most used sources to post on Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topsources_tweets = df.groupby(\"source\")[[\"tweets\"]].count()\n",
    "topsources_tweets.columns = [\"Number of Tweets\"]\n",
    "top10_sources = topsources_tweets.sort_values(\"Number of Tweets\", ascending=False).head(10)\n",
    "top10 = top10_sources.plot.barh(y=\"Number of Tweets\")\n",
    "top10_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see the length of the tweets. Keep in mind that on previous notebooks *1_Preprocessing_Tweets*, some tweets have been removed due to exceed length. For this reason, the maximun length is under 140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_long\"].plot(kind=\"hist\" , bins=20 , figsize=(15,5))\n",
    "plt.xlabel('tweet_long')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of long')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to explore more frequent location where people had post on March 8th and 9th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplocations_tweets = df.groupby(\"user_location\")[[\"tweets\"]].count().dropna(how=\"all\")\n",
    "toplocations_tweets.columns = [\"Number of Tweets\"]\n",
    "top10_locations = toplocations_tweets.sort_values(\"Number of Tweets\", ascending=False).head(10)\n",
    "top10_locations.plot.barh(y=\"Number of Tweets\")\n",
    "top10_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that Madrid and Argentina are the places with more activity on Twitter on 8th and 9th of March 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, there *user_location* is not normalizer because it is a text field were users can write down same location in different ways. \n",
    "\n",
    "For example:\n",
    "\n",
    "`Madrid, Comunidad de Madrid` is same as `Madrid`\n",
    "\n",
    "`Santiago, Chile` is equal to `Chile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_accent_mark(word):\n",
    "    s = ''.join((c for c in unicodedata.normalize('NFD',word) if unicodedata.category(c) != 'Mn'))\n",
    "    return s\n",
    "\n",
    "def get_hashtags(sentence):\n",
    "    hashtags = []\n",
    "    puntuation = string.punctuation.replace(\"#\", \"\") + \"¿¡…“”\"\n",
    "    for word in sentence.split():\n",
    "        if word.startswith(\"#\"):\n",
    "            word = word.lower()\n",
    "            word = re.sub('\\[.*?¿\\]\\%', ' ', word)\n",
    "            word = re.sub('[%s]' % re.escape(puntuation), '', word)\n",
    "            word = delete_accent_mark(word)\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to explore most common hashtags on March 8th and 9th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = list(map(lambda tweet: get_hashtags(tweet), df[\"tweets\"]))\n",
    "\n",
    "\n",
    "hashtags_tweets_general = []\n",
    "for item in hashtags:\n",
    "    for elem in item:\n",
    "        if elem not in hashtags_tweets_general:\n",
    "            hashtags_tweets_general.append(delete_accent_mark(elem))\n",
    "        \n",
    "hashtags_tweets_8m = []\n",
    "for item in hashtags:\n",
    "    for elem in item:\n",
    "        if not elem.startswith(\"#8M\") and not elem.startswith(\"#8m\") and (elem not in hashtags_tweets_8m):\n",
    "            hashtags_tweets_8m.append(delete_accent_mark(elem))\n",
    "\n",
    "            \n",
    "has    = \" \".join(h for h in hashtags_tweets_general)            \n",
    "has_8m = \" \".join(h for h in hashtags_tweets_8m)\n",
    "has_8m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent the most popular hashtags in a worldcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words(\"spanish\")\n",
    "wc_8m = WordCloud(width = 800, \n",
    "               height = 800, \n",
    "               background_color ='white', \n",
    "               stopwords = stop, \n",
    "               colormap =\"Dark2\",\n",
    "               min_font_size = 10,\n",
    "               max_font_size = 150,\n",
    "               random_state = 42,\n",
    "               normalize_plurals = True).generate(has_8m)\n",
    "\n",
    "# plot the WordCloud image  \n",
    "plt.imshow(wc_8m, interpolation ='bilinear')\n",
    "plt.rcParams['figure.figsize'] = [15,15]\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"8M Hashtags WordCloud\")\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words(\"spanish\")\n",
    "wc = WordCloud(width = 800, \n",
    "               height = 800, \n",
    "               background_color ='white', \n",
    "               stopwords = stop, \n",
    "               colormap =\"Dark2\",\n",
    "               min_font_size = 10,\n",
    "               max_font_size = 150,\n",
    "               random_state = 42,\n",
    "               normalize_plurals = True).generate(has)\n",
    "\n",
    "# plot the WordCloud image  \n",
    "plt.imshow(wc, interpolation ='bilinear')\n",
    "plt.rcParams['figure.figsize'] = [15,15]\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"General 8M Hashtags WordCloud\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the image into a .png file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.to_file(\"ejecuciones_codigo/8M_hashtags_wordcloud.png\")\n",
    "wc_8m.to_file(\"ejecuciones_codigo/General_8M_hashtags_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
